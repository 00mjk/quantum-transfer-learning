{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 5: Quantum-to-quantum transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of a continuous variable (CV) quantum network for state classification, developed according to the *quantum-to-quantum transfer learning* scheme presented in [1]. \n",
    "\n",
    "## Introduction \n",
    "\n",
    "\n",
    "In this proof-of-principle demonstration we consider two distinct toy datasets of Gaussian and non-Gaussian states. Such datasets can be generated according to the following simple prescriptions:\n",
    "\n",
    "**Dataset A**:  \n",
    "- Class 0 (Gaussian): random Gaussian layer applied to the vacuum.  \n",
    "- Class 1 (non-Gaussian): random non-Gaussian Layer applied to the vacuum.  \n",
    "        \n",
    "**Dataset B**:  \n",
    "- Class 0 (Gaussian): random Gaussian layer applied to a coherent state with amplitude $\\alpha=1$.\n",
    "- Class 1 (non-Gaussian): random Gaussian layer applied to a single photon Fock state $|1\\rangle$.\n",
    "\n",
    "**Variational Circuit A**:  \n",
    "Our starting point is a single-mode variational circuit [2] (a non-Gaussian layer), pre-trained on _Dataset A_. We assume that after the circuit is applied, the output mode is measured with an _on/off_ detector. By averaging over many shots, one can estimate the vacuum probability:\n",
    "\n",
    "$$\n",
    "p_0 = | \\langle \\psi_{\\rm out} |0 \\rangle|^2. \n",
    "$$\n",
    "\n",
    "We use _Dataset A_ and train the circuit to rotate Gaussian states towards the vacuum while non-Gaussian states far away from the vacuum. For the final classification we use the simple decision rule:\n",
    "\n",
    "$$\n",
    "p_0 \\ge 0  \\longrightarrow {\\rm Class=0.} \\\\\n",
    "p_0 < 0  \\longrightarrow {\\rm Class=1.}\n",
    "$$\n",
    "\n",
    "**Variational Circuit B**:  \n",
    "Once _Circuit A_ has been optimized, we can use is as a pre-trained block\n",
    "applicable also to the different _Dataset B_. In other words, we implement a _quantum-to-quantum_ transfer learning model:\n",
    "\n",
    "_Circuit B_ =  _Circuit A_ (pre-trained) followed by a sequence of  _variational layers_ (to be trained).\n",
    "\n",
    "Also in this case, after the application of _Circuit B_, we assume to measure the single mode with an _on/off_ detector, and we apply a similar classification rule:\n",
    "\n",
    "$$\n",
    "p_0 \\ge 0  \\longrightarrow {\\rm Class=1.} \\\\\n",
    "p_0 < 0  \\longrightarrow {\\rm Class=0.}\n",
    "$$\n",
    "\n",
    "The motivation for this transfer learning approach is that, even if _Circuit A_ is optimized on a different dataset, it can still act as a good pre-processing block also for _Dataset B_. Ineeed, as we are going to show, the application of _Circuit A_ can significantly improve the training efficiency of _Circuit B_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General setup\n",
    "\n",
    "The main imported modules are: the `tensorflow` machine learning framework, the quantum CV \n",
    "software `strawberryfields` [3] and the python plotting library `matplotlib`. All modules should be correctly installed in the system before running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Strawberryfields (simulation of CV quantum circuits)\n",
    "import strawberryfields as sf\n",
    "from strawberryfields.ops import Dgate, Kgate, Sgate, Rgate, Vgate, Fock, Ket\n",
    "\n",
    "# Other modules\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# System variables\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # avoid warning messages\n",
    "os.environ['OMP_NUM_THREADS'] = '1'       # set number of threads.\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'  # select the GPU unit.\n",
    "\n",
    "# Path with pre-trained parameters\n",
    "weights_path = 'results/weights/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting of the main parameters of the network model and of the training process.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilbert space cutoff\n",
    "cutoff = 15\n",
    "\n",
    "# Normalization cutoff (must be equal or smaller than cutoff dimension)\n",
    "target_cutoff = 15\n",
    "\n",
    "# Normalization weight\n",
    "norm_weight = 0\n",
    "\n",
    "# Batch size\n",
    "batch_size = 8\n",
    "\n",
    "# Number of batches (i.e. number training iterations)\n",
    "num_batches = 500\n",
    "\n",
    "# Number of state generation layers\n",
    "g_depth = 1\n",
    "\n",
    "# Number of pre-trained layers (for transfer learning)\n",
    "pre_depth = 1\n",
    "\n",
    "# Number of state classification layers\n",
    "q_depth = 3\n",
    "\n",
    "# Standard deviation of random state generation parameters\n",
    "rot_sd = np.math.pi * 2\n",
    "dis_sd = 0\n",
    "sq_sd = 0.5\n",
    "non_lin_sd = 0.5  # this is used as fixed non-linear constant.\n",
    "\n",
    "# Standard deviation of initial trainable weights\n",
    "active_sd = 0.001\n",
    "passive_sd = 0.001\n",
    "\n",
    "# Magnitude limit for trainable active parameters\n",
    "clip = 1 \n",
    "\n",
    "# Learning rate \n",
    "lr = 0.01 \n",
    "\n",
    "# Random seeds\n",
    "tf.set_random_seed(0)\n",
    "rng_data = np.random.RandomState(1)\n",
    "\n",
    "# Reset TF graph\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational circuits for state generation and classificaiton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input states: _Dataset B_\n",
    "\n",
    "The dataset is introduced by defining the corresponding random variational circuit that generates input Gaussian and non-Gaussian states.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders for class labels\n",
    "batch_labels = tf.placeholder(dtype=tf.int64, shape = [batch_size])\n",
    "batch_labels_fl = tf.to_float(batch_labels)\n",
    "\n",
    "# State generation parameters\n",
    "# Squeezing gate\n",
    "sq_gen = tf.placeholder(dtype = tf.float32, shape = [batch_size,g_depth])\n",
    "\n",
    "# Rotation gates        \n",
    "r1_gen = tf.placeholder(dtype = tf.float32, shape = [batch_size,g_depth])\n",
    "r2_gen = tf.placeholder(dtype = tf.float32, shape = [batch_size,g_depth])\n",
    "r3_gen = tf.placeholder(dtype = tf.float32, shape = [batch_size,g_depth])\n",
    "\n",
    "# Explicit definitions of the ket tensors of |0> and |1> \n",
    "np_ket0, np_ket1 = np.zeros((2, batch_size, cutoff))\n",
    "np_ket0[:,0] = 1.0\n",
    "np_ket1[:,1] = 1.0\n",
    "ket0 = tf.constant(np_ket0, dtype = tf.float32, shape = [batch_size, cutoff])\n",
    "ket1 = tf.constant(np_ket1, dtype = tf.float32, shape = [batch_size, cutoff])\n",
    "\n",
    "# Ket of the quantum states associated to the label: i.e. |batch_labels>\n",
    "ket_init = ket0 * (1.0 - tf.expand_dims(batch_labels_fl, 1)) + ket1 * tf.expand_dims(batch_labels_fl, 1)\n",
    "\n",
    "# State generation layer\n",
    "def layer_gen(i, qmode):\n",
    "    \n",
    "    # If label is 0 (Gaussian) prepare a coherent state with alpha=1 otherwise prepare fock |1>\n",
    "    Ket(ket_init) | qmode\n",
    "    Dgate((1.0 - batch_labels_fl) * 1.0, 0) | qmode\n",
    "    \n",
    "    # Random Gaussian operation (without displacement)\n",
    "    Rgate(r1_gen[:, i]) | qmode\n",
    "    Sgate(sq_gen[:, i], 0) | qmode\n",
    "    Rgate(r2_gen[:, i]) | qmode\n",
    "\n",
    "    return qmode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading of pre-trained block (_Circuit A_)\n",
    "\n",
    "We assume that _Circuit A_ has been already pre-trained (e.g. by running a dedicated Python script) and that the associated optimal weights have been saved to a NumPy file. Here we first load the such parameters and then we define _Circuit A_ as a constant pre-processing block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading of pre-trained weights\n",
    "trained_params_npy = np.load('pre_trained/circuit_A.npy')\n",
    "if trained_params_npy.shape[1] < pre_depth:\n",
    "                print(\"Error: circuit q_depth > trained q_depth.\")\n",
    "                raise SystemExit(0)\n",
    "            \n",
    "# Convert numpy arrays to TF tensors\n",
    "trained_params = tf.constant(trained_params_npy)\n",
    "    \n",
    "sq_pre = trained_params[0]\n",
    "d_pre = trained_params[1]\n",
    "r1_pre = trained_params[2]\n",
    "r2_pre = trained_params[3]\n",
    "r3_pre = trained_params[4]\n",
    "kappa_pre = trained_params[5]\n",
    "\n",
    "# Definition of the pre-trained Circuit A (single layer)\n",
    "\n",
    "def layer_pre(i, qmode):\n",
    "    \n",
    "    # Rotation gate\n",
    "    Rgate(r1_pre[i]) | qmode\n",
    "    # Squeezing gate\n",
    "    Sgate(tf.clip_by_value(sq_pre[i], -clip, clip), 0) \n",
    "    # Rotation gate\n",
    "    Rgate(r2_pre[i]) | qmode\n",
    "    # Displacement gate\n",
    "    Dgate(tf.clip_by_value(d_pre[i], -clip, clip) , 0) | qmode\n",
    "    # Rotation gate\n",
    "    Rgate(r3_pre[i]) | qmode\n",
    "    # Cubic gate\n",
    "    Vgate(tf.clip_by_value(kappa_pre[i], -clip, clip) ) | qmode\n",
    "    \n",
    "    return qmode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addition of trainable layers (_Circuit B_)\n",
    "\n",
    "As discussed in the introduction, _Circuit B_ can is obtained by adding some additional layers that we are going to train on _Dataset B_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainable variables\n",
    "with tf.name_scope('variables'):\n",
    "\n",
    "    # Squeeze gate\n",
    "    sq_var = tf.Variable(tf.random_normal(shape=[q_depth], stddev=active_sd))\n",
    "\n",
    "    # Displacement gate\n",
    "    d_var = tf.Variable(tf.random_normal(shape=[q_depth], stddev=active_sd))\n",
    "\n",
    "    # Rotation gates        \n",
    "    r1_var = tf.Variable(tf.random_normal(shape=[q_depth], stddev=passive_sd))\n",
    "    r2_var = tf.Variable(tf.random_normal(shape=[q_depth], stddev=passive_sd))\n",
    "    r3_var = tf.Variable(tf.random_normal(shape=[q_depth], stddev=passive_sd))\n",
    "\n",
    "    # Kerr gate\n",
    "    kappa_var = tf.Variable(tf.random_normal(shape=[q_depth], stddev=active_sd))\n",
    "\n",
    "    # 0-depth parameter (just to generate a gradient)\n",
    "    x_var = tf.Variable(0.0)\n",
    "\n",
    "parameters = [sq_var, d_var, r1_var, r2_var, r3_var, kappa_var]\n",
    "    \n",
    "# Definition of a single trainable variational layer\n",
    "def layer_var(i, qmode):\n",
    "\n",
    "    Rgate(r1_var[i]) | qmode\n",
    "    Sgate(tf.clip_by_value(sq_var[i], -clip, clip), 0) | qmode\n",
    "    Rgate(r2_var[i]) | qmode\n",
    "    Dgate(tf.clip_by_value(d_var[i], -clip, clip) , 0) | qmode\n",
    "    Rgate(r3_var[i]) | qmode\n",
    "    Vgate(tf.clip_by_value(kappa_var[i], -clip, clip) ) | qmode\n",
    "\n",
    "    return qmode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolic evaluation of the full network\n",
    "\n",
    "We first instantiate a _StrawberryFields_ quantum simulator, taylored for simulating a single-mode quantum optical system. Then we synbolically evaluate a batch of output states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prog = sf.Program(1)\n",
    "eng = sf.Engine('tf', backend_options={'cutoff_dim': cutoff, 'batch_size': batch_size})\n",
    "    \n",
    "# Circuit B                      \n",
    "with prog.context as q:\n",
    "     \n",
    "    # State generation network\n",
    "    for k in range(g_depth):\n",
    "        layer_gen(k, q[0])\n",
    "\n",
    "    # Pre-trained network (Circuit A)\n",
    "        for k in range(pre_depth):\n",
    "            layer_pre(k, q[0])\n",
    "\n",
    "    # State classification network\n",
    "    for k in range(q_depth):\n",
    "            layer_var(k, q[0])\n",
    "                      \n",
    "     # Special case q_depth==0               \n",
    "    if q_depth == 0:\n",
    "        Dgate(0.001, x_var ) | q[0] # almost identity operation just to generate a gradient.\n",
    "\n",
    "# Symbolic computation of the output state\n",
    "results = eng.run(prog, run_options={\"eval\": False})  \n",
    "out_state = results.state\n",
    "\n",
    "# Batch state norms\n",
    "out_norm = tf.to_float(out_state.trace())\n",
    "\n",
    "# Batch mean energies\n",
    "mean_n = out_state.mean_photon(0)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function, accuracy and optimizer.\n",
    "\n",
    "As usual in machine learning, we need to define a loss function that we are going to minimize during the training phase.\n",
    "\n",
    "As discussed in the introduction, we assume that only the vacuum state probability `p_0` is measured. Ideally, `p_0` should be large for non-Gaussian states (_label 1_), while should be small for Gaussian states (_label 0_). The circuit can be trained to this task by minimizing the _cross entropy_ loss function defined in the next cell.\n",
    "\n",
    "Moreover, if `norm_weight` is different from zero, also a regularization term is added to the full cost function in order to reduce quantum amplitudes beyond the target Hilbert space dimension `target_cutoff`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch vacuum probabilities\n",
    "p0 = out_state.fock_prob([0]) \n",
    "\n",
    "# Complementary probabilities\n",
    "q0 = 1.0 - p0\n",
    "\n",
    "# Cross entropy loss function\n",
    "eps = 0.0000001\n",
    "main_loss = tf.reduce_mean(-batch_labels_fl * tf.log(p0 + eps) - (1.0 - batch_labels_fl) * tf.log(q0 + eps))\n",
    "\n",
    "# Decision function\n",
    "predictions = tf.sign(p0 - 0.5) * 0.5 + 0.5\n",
    "\n",
    "# Accuracy between predictions and labels\n",
    "accuracy = tf.reduce_mean((predictions + batch_labels_fl - 1.0) ** 2)\n",
    "\n",
    "# Norm loss. This is monitored but not minimized.\n",
    "norm_loss = tf.reduce_mean((out_norm - 1.0) ** 2)\n",
    "\n",
    "# Cutoff loss regularization. This is monitored and minimized if norm_weight is nonzero.\n",
    "c_in = out_state.all_fock_probs()\n",
    "cut_probs = c_in[:, :target_cutoff]\n",
    "cut_norms = tf.reduce_sum(cut_probs, axis=1)\n",
    "cutoff_loss = tf.reduce_mean((cut_norms - 1.0) ** 2 ) \n",
    "\n",
    "# Full regularized loss function\n",
    "full_loss = main_loss + norm_weight * cutoff_loss\n",
    "\n",
    "# Optimization algorithm\n",
    "optim = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "training = optim.minimize(full_loss)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now we just defined the analytic graph of the quantum network without numerically evaluating it. Now, after initializing a _TensorFlow_ session, we can finally run the actual training and testing phases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch: 100, Running loss: 0.6885, Running acc 0.3700, Norm loss 0.0460, Batch time 0.0494\n",
      "Train batch: 200, Running loss: 0.6673, Running acc 0.3750, Norm loss 0.0599, Batch time 0.0498\n",
      "Train batch: 300, Running loss: 0.6575, Running acc 0.3825, Norm loss 0.0495, Batch time 0.0502\n",
      "Train batch: 400, Running loss: 0.6463, Running acc 0.3975, Norm loss 0.1070, Batch time 0.0497\n",
      "Train batch: 500, Running loss: 0.6113, Running acc 0.4765, Norm loss 0.1387, Batch time 0.0862\n",
      "Test batch: 100, Running loss: 0.4438, Running acc 0.8762, Norm loss 0.0613, Batch time 0.0206\n",
      "Test batch: 200, Running loss: 0.4376, Running acc 0.8825, Norm loss 0.0801, Batch time 0.0202\n",
      "Test batch: 300, Running loss: 0.4345, Running acc 0.8808, Norm loss 0.0391, Batch time 0.0199\n",
      "Test batch: 400, Running loss: 0.4359, Running acc 0.8775, Norm loss 0.0698, Batch time 0.0260\n",
      "Test batch: 500, Running loss: 0.4354, Running acc 0.8755, Norm loss 0.1015, Batch time 0.0256\n",
      "Training and testing phases completed.\n",
      "RESULTS:\n",
      " train_loss  train_acc  test_loss   test_acc  norm_loss     mean_n\n",
      "   0.611272   0.476500   0.435440   0.875500   0.101467   5.336512\n"
     ]
    }
   ],
   "source": [
    "# Function generating a dictionary of random parameters for a batch of states.\n",
    "def random_dict():\n",
    "    param_dict = {  # Labels (0 = Gaussian, 1 = non-Gaussian)\n",
    "                    batch_labels: rng_data.randint(2, size=batch_size),\n",
    "        \n",
    "                    # Squeezing and rotation parameters \n",
    "                    sq_gen: rng_data.uniform(low=-sq_sd, high=sq_sd, size=[batch_size, g_depth]),\n",
    "                    r1_gen: rng_data.uniform(low=-rot_sd, high=rot_sd, size=[batch_size, g_depth]),\n",
    "                    r2_gen: rng_data.uniform(low=-rot_sd, high=rot_sd, size=[batch_size, g_depth]),\n",
    "                    r3_gen: rng_data.uniform(low=-rot_sd, high=rot_sd, size=[batch_size, g_depth]),\n",
    "                 }\n",
    "    return param_dict\n",
    "\n",
    "# TensorFlow session\n",
    "with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "\n",
    "        train_loss = 0.0\n",
    "        train_loss_sum = 0.0\n",
    "        train_acc = 0.0\n",
    "        train_acc_sum = 0.0\n",
    "\n",
    "        test_loss = 0.0\n",
    "        test_loss_sum = 0.0\n",
    "        test_acc = 0.0\n",
    "        test_acc_sum = 0.0\n",
    "\n",
    "        # =========================================================\n",
    "        #  \tTraining Phase \n",
    "        # =========================================================\n",
    "        \n",
    "        if q_depth > 0:\n",
    "            for k in range(num_batches):\n",
    "                rep_time = time.time()\n",
    "                \n",
    "                # Training step\n",
    "                [_training,\n",
    "                _full_loss,\n",
    "                _accuracy,\n",
    "                _norm_loss] = session.run([ training,\n",
    "                                            full_loss,\n",
    "                                            accuracy,\n",
    "                                            norm_loss], feed_dict=random_dict())\n",
    "                train_loss_sum += _full_loss\n",
    "                train_acc_sum += _accuracy\n",
    "                train_loss = train_loss_sum / (k + 1)\n",
    "                train_acc = train_acc_sum / (k + 1)\n",
    "                \n",
    "                # Training log\n",
    "                if ((k + 1) % 100) == 0:\n",
    "                    print('Train batch: {:d}, Running loss: {:.4f}, Running acc {:.4f}, Norm loss {:.4f}, Batch time {:.4f}'\n",
    "                            .format(k + 1, train_loss, train_acc, _norm_loss, time.time() - rep_time))\n",
    "\n",
    "        # =========================================================\n",
    "        #  \tTesting Phase \n",
    "        # =========================================================\n",
    "        num_test_batches = min(num_batches, 1000)\n",
    "\n",
    "        for i in range(num_test_batches):  \n",
    "            rep_time = time.time()\n",
    "            \n",
    "            # Evaluation step\n",
    "            [_full_loss,\n",
    "            _accuracy,\n",
    "            _norm_loss,\n",
    "            _cutoff_loss,\n",
    "            _mean_n,\n",
    "            _parameters] = session.run([full_loss,\n",
    "                                        accuracy,\n",
    "                                        norm_loss,\n",
    "                                        cutoff_loss,\n",
    "                                        mean_n,\n",
    "                                        parameters], feed_dict=random_dict())\n",
    "            test_loss_sum += _full_loss\n",
    "            test_acc_sum += _accuracy\n",
    "            test_loss = test_loss_sum / (i + 1)\n",
    "            test_acc = test_acc_sum / (i + 1)\n",
    "            \n",
    "            # Testing log\n",
    "            if ((i + 1) % 100) == 0:\n",
    "                print('Test batch: {:d}, Running loss: {:.4f}, Running acc {:.4f}, Norm loss {:.4f}, Batch time {:.4f}'\n",
    "                      .format(i + 1, test_loss, test_acc, _norm_loss, time.time() - rep_time))\n",
    "\n",
    "# Compute mean photon number of the last batch of states\n",
    "mean_fock = np.mean(_mean_n)\n",
    "        \n",
    "print('Training and testing phases completed.')\n",
    "print('RESULTS:')\n",
    "print('{:>11s}{:>11s}{:>11s}{:>11s}{:>11s}{:>11s}'.format('train_loss', 'train_acc', 'test_loss', 'test_acc', 'norm_loss', 'mean_n'))\n",
    "print('{:11f}{:11f}{:11f}{:11f}{:11f}{:11f}'.format(train_loss, train_acc, test_loss, test_acc, _norm_loss, mean_fock))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Andrea Mari, Thomas R. Bromley, Josh Izaac, Maria Schuld, and Nathan Killoran.  _Transfer learning in hybrid classical-quantum neural networks_. [arXiv:xxxx.xxxx](https://arxiv.org/abs/xxxx.xxxx), (2019).\n",
    "\n",
    "[2] Nathan Killoran, Thomas R. Bromley, Juan Miguel Arrazola, Maria Schuld, Nicolás Quesada, and Seth Lloyd. _Continuous-variable quantum neural networks_. [arXiv:1806.06871](https://arxiv.org/abs/1806.06871), (2018).\n",
    "\n",
    "[3] Nathan Killoran, Josh Izaac, Nicolás Quesada, Ville Bergholm, Matthew Amy, and Christian Weedbrook. _Strawberry Fields: A Software Platform for Photonic Quantum Computing_. [Quantum, 3, 129 (2019)](https://doi.org/10.22331/q-2019-03-11-129)."
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
